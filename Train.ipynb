{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8kYmAkSSrlP2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from models import StateEncoder, OffsetEncoder, TargetEncoder, LSTM, Decoder, ShortMotionDiscriminator, LongMotionDiscriminator\n",
    "from skeleton.skeleton import Skeleton\n",
    "from functions import gen_ztta\n",
    "import config\n",
    "import LaFan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(LaFan)\n",
    "from config import *\n",
    "from LaFan import LaFan1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "P0jtHU74rlP4"
   },
   "outputs": [],
   "source": [
    "# --- Generator ---\n",
    "state_encoder = StateEncoder(in_dim=model[\"state_input_dim\"]) # 95: 22 * 4 (quaternions) + 4 (contact) + 3 (root position)\n",
    "state_encoder = state_encoder.to(device)\n",
    "\n",
    "offset_encoder = OffsetEncoder(in_dim=model[\"offset_input_dim\"]) # 91: 22 * 4 (quaternions) + 3 (root position)\n",
    "offset_encoder = offset_encoder.to(device)\n",
    "\n",
    "target_encoder = TargetEncoder(in_dim=model[\"target_input_dim\"]) # 88: 22 * 4 (quaternions)\n",
    "target_encoder = target_encoder.to(device)\n",
    "\n",
    "lstm = LSTM(in_dim=model[\"lstm_dim\"], hidden_dim=model[\"lstm_dim\"] * 2)\n",
    "lstm = lstm.to(device)\n",
    "\n",
    "decoder = Decoder(in_dim=model[\"lstm_dim\"] * 2, out_dim=model[\"decoder_output_dim\"]) # 95\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongMotionDiscriminator(\n",
       "  (fc0): Conv1d(132, 512, kernel_size=(10,), stride=(1,))\n",
       "  (fc1): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "  (fc2): Conv1d(256, 1, kernel_size=(1,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Discriminators ---\n",
    "short_discriminator = ShortMotionDiscriminator(in_dim=model['num_joints'] * 3 * 2)\n",
    "short_discriminator.to(device)\n",
    "\n",
    "long_discriminator = LongMotionDiscriminator(in_dim=model['num_joints'] * 3 * 2)\n",
    "long_discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UaPlXZUdrlP4"
   },
   "outputs": [],
   "source": [
    "# --- Skeleton ---\n",
    "skeleton_mocap = Skeleton(offsets=data[\"offsets\"], parents=data[\"parents\"])\n",
    "skeleton_mocap.to(device)\n",
    "skeleton_mocap.remove_joints(data[\"joints_to_remove\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hd0a47YBrlP5",
    "outputId": "d5017728-6b88-45d5-a712-59b5b2d22996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the data set... ['subject1', 'subject2', 'subject3', 'subject4']\n",
      "Processing file dance2_subject2.bvh\n",
      "Processing file dance2_subject4.bvh\n",
      "Processing file dance2_subject1.bvh\n",
      "Processing file dance2_subject3.bvh\n",
      "Nb of sequences : 448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lafan = LaFan1(data[\"path_small_flipped\"], seq_len=data[\"seq_length\"], offset=data[\"offset\"], train=True, debug=False)\n",
    "x_mean = lafan.x_mean.to(device)\n",
    "x_std = lafan.x_std.to(device).view(1, 1, 22, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "z3saZLLYsF-x"
   },
   "outputs": [],
   "source": [
    "lafan_loader = DataLoader(lafan, batch_size=train[\"batch_size\"], shuffle=True, num_workers=data[\"num_workers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8pA5hXW4rlP6"
   },
   "outputs": [],
   "source": [
    "# --- Optimizer ---\n",
    "optimizer_g = optim.Adam(lr=train[\"lr\"], params=list(state_encoder.parameters()) +\\\n",
    "                                            list(offset_encoder.parameters()) +\\\n",
    "                                            list(target_encoder.parameters()) +\\\n",
    "                                            list(lstm.parameters()) +\\\n",
    "                                            list(decoder.parameters()), \\\n",
    "                                            betas=(train['beta1'], train['beta2']), \\\n",
    "                                            weight_decay=train['weight_decay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_d = optim.Adam(lr=train['lr'], params=list(short_discriminator.parameters()) +\\\n",
    "                                             list(long_discriminator.parameters()), \\\n",
    "                                             betas = (train['beta1'], train['beta2']), \\\n",
    "                                             weight_decay = train['weight_decay'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"small_dataset_test_00\"\n",
    "writer = SummaryWriter(f\"logs/{experiment}\")\n",
    "log_i = 0\n",
    "save_logs = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progressive Training\n",
    "\n",
    "Total epochs trained: 250\n",
    "Max sequence length: 150 (5 seconds)\n",
    "Starting min sequence for training: 30\n",
    "Increasing by 5 frames every 5 epochs\n",
    "\n",
    "epoch 1 - 30\n",
    "epoch 6 - 35\n",
    "epoch 11 - 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (fc0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=91, bias=True)\n",
       "  (fc_conct): Linear(in_features=256, out_features=4, bias=True)\n",
       "  (ac_sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_encoder.train()\n",
    "offset_encoder.train()\n",
    "target_encoder.train()\n",
    "lstm.train()\n",
    "decoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DbNMLHyJrlP7",
    "outputId": "7a09c22c-dbd4-48e0-fd49-9ebe14953503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EPOCH 0 ---\n",
      "torch.Size([128, 30, 22, 4])\n",
      "torch.Size([128, 29, 3])\n",
      "torch.Size([128, 30, 4])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/usama/Projects/RobustMotionInBetweening/Train.ipynb Cell 17'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/usama/Projects/RobustMotionInBetweening/Train.ipynb#ch0000014?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mroot_v\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/usama/Projects/RobustMotionInBetweening/Train.ipynb#ch0000014?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcontact\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/usama/Projects/RobustMotionInBetweening/Train.ipynb#ch0000014?line=24'>25</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/usama/Projects/RobustMotionInBetweening/Train.ipynb#ch0000014?line=26'>27</a>\u001b[0m \u001b[39m# Offset inputs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/usama/Projects/RobustMotionInBetweening/Train.ipynb#ch0000014?line=27'>28</a>\u001b[0m root_p_offset \u001b[39m=\u001b[39m sampled_batch[\u001b[39m\"\u001b[39m\u001b[39mroot_p_offset\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)               \u001b[39m# batch_sample, root postion on last frame\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    print(f\"\\n--- EPOCH {epoch} ---\")\n",
    "\n",
    "    p = min(30 + 5 * (epoch // 5), 150)\n",
    "\n",
    "    # Z-time to arrival\n",
    "    ztta = gen_ztta(length=data[\"seq_length\"]).to(device)                       # 1, number of frames, output size of encoders (256)\n",
    "\n",
    "    for i_batch, sampled_batch in enumerate(lafan_loader):\n",
    "        # Loss\n",
    "        loss_pos = 0\n",
    "        loss_quat = 0\n",
    "        loss_contact = 0\n",
    "        loss_root = 0\n",
    "\n",
    "        # State inputs\n",
    "        local_q = sampled_batch[\"local_q\"][:, :p].to(device)                    # batch_sample, t, joint, quaternion\n",
    "        root_v = sampled_batch[\"root_v\"][:, :p - 1].to(device)                  # batch_sample, t-1, velocity\n",
    "        contact = sampled_batch[\"contact\"][:, :p].to(device)                    # batch_sample, t, contact\n",
    "\n",
    "        # Offset inputs\n",
    "        root_p_offset = sampled_batch[\"root_p_offset\"][:, p].to(device)         # batch_sample, root postion on last frame\n",
    "        local_q_offset = sampled_batch[\"local_q_offset\"][:, p].to(device)       # batch_sample, quaternions of all joints on last frame\n",
    "        local_q_offset = local_q_offset.view(local_q_offset.size(0), -1)        # Flatten with joint x quaternions\n",
    "\n",
    "        # Target inputs\n",
    "        target = sampled_batch[\"target\"][:, p].to(device)                       # batch_sample, quaternions of all joints on last frame\n",
    "        target = target.view(target.size(0), -1)                                # Flatten with joint x quaternions\n",
    "\n",
    "        # Root position\n",
    "        root_p = sampled_batch[\"root_p\"][:, :p].to(device)                      # batch_sample, t, root_position\n",
    "\n",
    "        # X\n",
    "        X = sampled_batch[\"X\"][:, :p].to(device)                                # batch_sample, t, joint, position\n",
    "\n",
    "        lstm.init_hidden(local_q.size(0))\n",
    "        pred_list = []\n",
    "        pred_list.append(X[:, 0])                                               # First frame quaternions for all joints\n",
    "\n",
    "        root_pred = None\n",
    "        local_q_pred = None\n",
    "        contact_pred = None\n",
    "        root_v_pred = None\n",
    "\n",
    "        for t in range(lafan.cur_seq_length - 1):\n",
    "            if t == 0:\n",
    "                root_p_t = root_p[:, t]                                         # batch_sample, position at time t\n",
    "                local_q_t = local_q[:,t]                                        # batch_sample, joint, quaternions at time t\n",
    "                local_q_t = local_q_t.view(local_q_t.size(0), -1)               # batch_sample, quaternions for each joint flattened at time t\n",
    "                contact_t = contact[:, t]                                       # batch_sample, contact at time t\n",
    "                root_v_t = root_v[:, t]                                         # batch_sample, velocity at time t\n",
    "            else:\n",
    "                # Getting 0th index as predictions have a dimension added (unsqueezing)\n",
    "                root_p_t = root_pred[0]\n",
    "                local_q_t = local_q_pred[0]\n",
    "                contact_t = contact_pred[0]\n",
    "                root_v_t = root_v_pred[0]\n",
    "\n",
    "            # State vector\n",
    "            state_input = torch.cat([local_q_t, root_v_t, contact_t], -1)       # 88 + 3 + 4 = 95 \n",
    "\n",
    "            # Offset vector\n",
    "            root_p_offset_t = root_p_offset - root_p_t                          # last frame root position - current root position\n",
    "            local_q_offset_t = local_q_offset - local_q_t                       # last frame quaternions - current frame quaternions (for all joints)\n",
    "            offset_input = torch.cat([root_p_offset_t, local_q_offset_t], -1)   # 3 + 88 = 91\n",
    "\n",
    "            # Target vector\n",
    "            target_input = target                                               # quaternions of all joints on last frame\n",
    "\n",
    "            # Passing vectors through encoders\n",
    "            h_state = state_encoder(state_input)\n",
    "            h_offset = offset_encoder(offset_input)\n",
    "            h_target = target_encoder(target_input)\n",
    "\n",
    "            h_state += ztta[:, t]\n",
    "            h_offset += ztta[:, t]\n",
    "            h_target += ztta[:, t]\n",
    "            \n",
    "            # Scheduled target noise\n",
    "            tta = lafan.cur_seq_length - 2 - t\n",
    "            if tta < 5:\n",
    "                lambda_target = 0.0\n",
    "            elif tta >= 5 and tta < 30:\n",
    "                lambda_target = (tta - 5) / 25.0\n",
    "            else:\n",
    "                lambda_target = 1.0\n",
    "            h_offset += 0.5 * lambda_target * torch.FloatTensor(h_offset.size()).normal_().to(device)\n",
    "            h_target += 0.5 * lambda_target * torch.FloatTensor(h_target.size()).normal_().to(device)\n",
    "\n",
    "            # Passing encoder outputs to LSTM\n",
    "            lstm_input = torch.cat([h_state, h_offset, h_target], -1).unsqueeze(0)\n",
    "            h_out = lstm(lstm_input)\n",
    "\n",
    "            # Passing LSTM output to decoder\n",
    "            h_pred, contact_pred = decoder(h_out)                               # decoder returns (change in quaternions + root velocity change), contact predictions\n",
    "        \n",
    "            # Calculating quaternions at time t + 1\n",
    "            local_q_v_pred = h_pred[:, :, :model[\"target_input_dim\"]]\n",
    "            local_q_pred = local_q_v_pred + local_q_t\n",
    "\n",
    "            # Unflattening\n",
    "            local_q_pred_ = local_q_pred.view(local_q_pred.size(0), local_q_pred.size(1), -1, 4)\n",
    "            local_q_pred_ = local_q_pred_ / torch.norm(local_q_pred_, dim = -1, keepdim = True) # ?\n",
    "\n",
    "            # Calculating root position at time t + 1\n",
    "            root_v_pred = h_pred[:, :, model[\"target_input_dim\"]:]\n",
    "            root_pred = root_v_pred + root_p_t\n",
    "            \n",
    "            # Calculating positions\n",
    "            pos_pred = skeleton_mocap.forward_kinematics(local_q_pred_, root_pred)\n",
    "\n",
    "            # Desired values, for calculating loss\n",
    "            pos_next = X[:,t+1]\n",
    "            local_q_next = local_q[:,t+1]\n",
    "            local_q_next = local_q_next.view(local_q_next.size(0), -1)\n",
    "            root_p_next = root_p[:,t+1]\n",
    "            contact_next = contact[:,t+1]\n",
    "\n",
    "            # Calculating loss\n",
    "            loss_pos += torch.mean(torch.abs(pos_pred[0] - pos_next) / x_std) / lafan.cur_seq_length\n",
    "            loss_quat += torch.mean(torch.abs(local_q_pred[0] - local_q_next)) / lafan.cur_seq_length\n",
    "            loss_root += torch.mean(torch.abs(root_pred[0] - root_p_next) / x_std[:,:,0]) / lafan.cur_seq_length\n",
    "            loss_contact += torch.mean(torch.abs(contact_pred[0] - contact_next)) / lafan.cur_seq_length\n",
    "            pred_list.append(pos_pred[0])\n",
    "            \n",
    "        # Training Discriminator\n",
    "        fake_input = torch.cat([x.reshape(x.size(0), -1).unsqueeze(-1) for x in pred_list], -1)\n",
    "        fake_v_input = torch.cat([fake_input[:,:,1:] - fake_input[:,:,:-1], torch.zeros_like(fake_input[:,:,0:1]).to(device)], -1)\n",
    "        fake_input = torch.cat([fake_input, fake_v_input], 1)\n",
    "\n",
    "        real_input = torch.cat([X[:, i].view(X.size(0), -1).unsqueeze(-1) for i in range(lafan.cur_seq_length)], -1)\n",
    "        real_v_input = torch.cat([real_input[:,:,1:] - real_input[:,:,:-1], torch.zeros_like(real_input[:,:,0:1]).to(device)], -1)\n",
    "        real_input = torch.cat([real_input, real_v_input], 1)\n",
    "\n",
    "        optimizer_d.zero_grad()\n",
    "        short_fake_logits = torch.mean(short_discriminator(fake_input.detach())[:, 0], 1)\n",
    "        short_real_logits = torch.mean(short_discriminator(real_input)[:, 0], 1)\n",
    "        short_d_fake_loss = torch.mean((short_fake_logits) ** 2)\n",
    "        short_d_real_loss = torch.mean((short_real_logits -  1) ** 2)\n",
    "        short_d_loss = (short_d_fake_loss + short_d_real_loss) / 2.0\n",
    "                \n",
    "        long_fake_logits = torch.mean(long_discriminator(fake_input.detach())[:,0], 1)\n",
    "        long_real_logits = torch.mean(long_discriminator(real_input)[:,0], 1)\n",
    "        long_d_fake_loss = torch.mean((long_fake_logits) ** 2)\n",
    "        long_d_real_loss = torch.mean((long_real_logits -  1) ** 2)\n",
    "        long_d_loss = (long_d_fake_loss + long_d_real_loss) / 2.0\n",
    "                    \n",
    "        total_d_loss = train['loss_adv_weight'] * long_d_loss + \\\n",
    "                       train['loss_adv_weight'] * short_d_loss\n",
    "        total_d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "            \n",
    "        # Backprop\n",
    "        optimizer_g.zero_grad()\n",
    "        pred_pos = torch.cat([x.reshape(x.size(0), -1).unsqueeze(-1) for x in pred_list], -1)\n",
    "        pred_vel = (pred_pos[:, data[\"foot_index\"], 1:] - pred_pos[:, data[\"foot_index\"], :-1])\n",
    "        pred_vel = pred_vel.view(pred_vel.size(0), 4, 3, pred_vel.size(-1))\n",
    "        loss_slide = torch.mean(torch.abs(pred_vel * contact[:,:-1].permute(0, 2, 1).unsqueeze(2)))\n",
    "        loss_total = train[\"loss_pos_weight\"] * loss_pos + \\\n",
    "                    train[\"loss_quat_weight\"] * loss_quat + \\\n",
    "                    train[\"loss_root_weight\"] * loss_root + \\\n",
    "                    train[\"loss_slide_weight\"] * loss_slide + \\\n",
    "                    train[\"loss_contact_weight\"] * loss_contact\n",
    "        \n",
    "        short_fake_logits = torch.mean(short_discriminator(fake_input)[:, 0], 1)\n",
    "        short_g_loss = torch.mean((short_fake_logits - 1) ** 2)\n",
    "        long_fake_logits = torch.mean(long_discriminator(fake_input)[:, 0], 1)\n",
    "        long_g_loss = torch.mean((long_fake_logits -1) ** 2)\n",
    "        total_g_loss = train['loss_adv_weight'] * long_g_loss + \\\n",
    "                       train['loss_adv_weight'] * short_g_loss\n",
    "        loss_total += total_g_loss\n",
    "\n",
    "        loss_total.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(state_encoder.parameters(), 0.5)\n",
    "        torch.nn.utils.clip_grad_norm_(offset_encoder.parameters(), 0.5)\n",
    "        torch.nn.utils.clip_grad_norm_(target_encoder.parameters(), 0.5)\n",
    "        torch.nn.utils.clip_grad_norm_(lstm.parameters(), 0.5)\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), 0.5)\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        if i_batch % 1 == 0: \n",
    "            print(f\"Epoch {epoch}, Batch {i_batch}\")\n",
    "            print(f\"\\tTotal Loss: {loss_total}\")\n",
    "            print()\n",
    "\n",
    "    # Loggign Loss\n",
    "    if save_logs:\n",
    "        writer.add_scalar(\"Loss/Pos\", loss_pos.item(), log_i)\n",
    "        writer.add_scalar(\"Loss/Quat\", loss_quat.item(), log_i)\n",
    "        writer.add_scalar(\"Loss/Root\", loss_root.item(), log_i)\n",
    "        writer.add_scalar(\"Loss/Slide\", loss_slide.item(), log_i)\n",
    "        writer.add_scalar(\"Loss/Contact\", loss_contact.item(), log_i)\n",
    "        writer.add_scalar(\"Loss/Total\", loss_total.item(), log_i)\n",
    "        writer.add_scalar(\"Adversarial Loss/Short Generator\", short_g_loss.item(), log_i)\n",
    "        writer.add_scalar(\"Adversarial Loss/Long Generator\", long_g_loss.item(), log_i)\n",
    "        writer.add_scalar(\"Adversarial Loss/Short Discriminator Real\", short_d_real_loss.item(), log_i)\n",
    "        writer.add_scalar(\"Adversarial Loss/Short Discriminator Fake\", short_d_fake_loss.item(), log_i)\n",
    "        writer.add_scalar(\"Adversarial Loss/Long Discriminator Real\", long_d_real_loss.item(), log_i)\n",
    "        writer.add_scalar(\"Adversarial Loss/Long Discriminator Fake\", long_d_fake_loss.item(), log_i)\n",
    "        log_i += 1\n",
    "\n",
    "    # Saving models\n",
    "    if (epoch != 0 and epoch % 1 == 0):\n",
    "        folder_name = f\"./models/{experiment}/epoch_{epoch}\"\n",
    "        os.makedirs(folder_name, exist_ok=True)\n",
    "        torch.save(state_encoder.state_dict(), f\"{folder_name}/state_encoder.pkl\")\n",
    "        torch.save(target_encoder.state_dict(), f\"{folder_name}/target_encoder.pkl\")\n",
    "        torch.save(offset_encoder.state_dict(), f\"{folder_name}/offset_encoder.pkl\")\n",
    "        torch.save(lstm.state_dict(), f\"{folder_name}/lstm.pkl\")\n",
    "        torch.save(decoder.state_dict(), f\"{folder_name}/decoder.pkl\")\n",
    "        torch.save(optimizer_g.state_dict(), f\"{folder_name}/optimizer_g.pkl\")\n",
    "        \n",
    "        torch.save(short_discriminator.state_dict(), f\"{folder_name}/short_discriminator.pkl\")\n",
    "        torch.save(long_discriminator.state_dict(), f\"{folder_name}/long_discriminator.pkl\")\n",
    "        torch.save(optimizer_d.state_dict(), f\"{folder_name}/optimizer_d.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of train.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "ae45820c86b1fd4170b01beeadd0fb73f4368fb2ccf31f4fd997ede2de2c254f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
