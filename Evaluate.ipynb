{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dac96f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import moviepy.video.io.ImageSequenceClip\n",
    "from functions import gen_ztta\n",
    "from models import StateEncoder, OffsetEncoder, TargetEncoder, LSTM, Decoder\n",
    "from skeleton.skeleton import Skeleton\n",
    "from LaFan import LaFan1\n",
    "from functions import gen_ztta\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8355da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import functions\n",
    "importlib.reload(functions)\n",
    "from functions import gen_ztta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e78c494",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e671b0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './lafan1/lafan1/flipped/aiming1_subject1.bvh'\n",
    "start = 1000\n",
    "sequence_length = 60\n",
    "end = start + sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81dbba88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence loaded, length 7184 frames.\n",
      "Slice of length 60 returned\n"
     ]
    }
   ],
   "source": [
    "sequence = LaFan1.load_single_bvh_sequence(filepath, start=start, end=end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a424717d",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bedd261",
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton = Skeleton(offsets=data[\"offsets\"], parents=data[\"parents\"])\n",
    "skeleton.to(device)\n",
    "skeleton.remove_joints(data[\"joints_to_remove\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be12d5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_folder = \"./models/sub1234_2sec_trial_4/epoch_200/\"\n",
    "\n",
    "state_encoder = StateEncoder(in_dim=model[\"state_input_dim\"])\n",
    "state_encoder = state_encoder.to(device)\n",
    "state_encoder.load_state_dict(torch.load(f\"{models_folder}/state_encoder.pkl\", map_location=torch.device('cpu') ))\n",
    "\n",
    "offset_encoder = OffsetEncoder(in_dim=model[\"offset_input_dim\"])\n",
    "offset_encoder = offset_encoder.to(device)\n",
    "offset_encoder.load_state_dict(torch.load(f\"{models_folder}/offset_encoder.pkl\", map_location=torch.device('cpu') ))\n",
    "\n",
    "target_encoder = TargetEncoder(in_dim=model[\"target_input_dim\"])\n",
    "target_encoder = target_encoder.to(device)\n",
    "target_encoder.load_state_dict(torch.load(f\"{models_folder}/target_encoder.pkl\", map_location=torch.device('cpu') ))\n",
    "\n",
    "lstm = LSTM(in_dim=model[\"lstm_dim\"], hidden_dim=model[\"lstm_dim\"] * 2)\n",
    "lstm = lstm.to(device)\n",
    "lstm.load_state_dict(torch.load(f\"{models_folder}/lstm.pkl\", map_location=torch.device('cpu') ))\n",
    "\n",
    "decoder = Decoder(in_dim=model[\"lstm_dim\"]*2, out_dim=model[\"decoder_output_dim\"])\n",
    "decoder = decoder.to(device)\n",
    "decoder.load_state_dict(torch.load(f\"{models_folder}/decoder.pkl\", map_location=torch.device('cpu') ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9316dd",
   "metadata": {},
   "source": [
    "# Rendering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a63bd5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pose(pose, cur_frame, prefix):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    parents = [-1, 0, 1, 2, 3, 0, 5, 6, 7, 0, 9, 10, 11, 12, 11, 14, 15, 16, 11, 18, 19, 20]\n",
    "    ax.cla()\n",
    "    num_joint = pose.shape[0] // 3\n",
    "    for i, p in enumerate(parents):\n",
    "        if i > 0:\n",
    "            ax.plot([pose[i, 0], pose[p, 0]],\\\n",
    "                    [pose[i, 2], pose[p, 2]],\\\n",
    "                    [pose[i, 1], pose[p, 1]], c='r')\n",
    "            ax.plot([pose[i+num_joint, 0], pose[p+num_joint, 0]],\\\n",
    "                    [pose[i+num_joint, 2], pose[p+num_joint, 2]],\\\n",
    "                    [pose[i+num_joint, 1], pose[p+num_joint, 1]], c='b')\n",
    "            ax.plot([pose[i+num_joint*2, 0], pose[p+num_joint*2, 0]],\\\n",
    "                    [pose[i+num_joint*2, 2], pose[p+num_joint*2, 2]],\\\n",
    "                    [pose[i+num_joint*2, 1], pose[p+num_joint*2, 1]], c='g')\n",
    "    ax.scatter(pose[:num_joint, 0], pose[:num_joint, 2], pose[:num_joint, 1],c='b')\n",
    "    ax.scatter(pose[num_joint:num_joint*2, 0], pose[num_joint:num_joint*2, 2], pose[num_joint:num_joint*2, 1],c='b')\n",
    "    ax.scatter(pose[num_joint*2:num_joint*3, 0], pose[num_joint*2:num_joint*3, 2], pose[num_joint*2:num_joint*3, 1],c='g')\n",
    "    xmin = np.min(pose[:, 0])\n",
    "    ymin = np.min(pose[:, 2])\n",
    "    zmin = np.min(pose[:, 1])\n",
    "    xmax = np.max(pose[:, 0])\n",
    "    ymax = np.max(pose[:, 2])\n",
    "    zmax = np.max(pose[:, 1])\n",
    "    scale = np.max([xmax - xmin, ymax - ymin, zmax - zmin])\n",
    "    xmid = (xmax + xmin) // 2\n",
    "    ymid = (ymax + ymin) // 2\n",
    "    zmid = (zmax + zmin) // 2\n",
    "    ax.set_xlim(xmid - scale // 2, xmid + scale // 2)\n",
    "    ax.set_ylim(ymid - scale // 2, ymid + scale // 2)\n",
    "    ax.set_zlim(zmid - scale // 2, zmid + scale // 2)\n",
    "\n",
    "    plt.draw()\n",
    "    plt.savefig(f\"{prefix}_{cur_frame:02}.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2a91d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video(frames_loc, filepath):\n",
    "    fps = 30\n",
    "    \n",
    "    frames = [os.path.join(frames_loc, img) for img in os.listdir(frames_loc) if img.endswith(\".png\") and img.startswith(\"pred\")]\n",
    "    frames.sort()\n",
    "    clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(frames, fps=fps)\n",
    "    clip.write_videofile(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c63e9",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b066da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ztta = gen_ztta(timesteps=data[\"seq_length\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fddfbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (fc0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=91, bias=True)\n",
       "  (fc_conct): Linear(in_features=256, out_features=4, bias=True)\n",
       "  (ac_sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_encoder.eval()\n",
    "offset_encoder.eval()\n",
    "target_encoder.eval()\n",
    "lstm.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f4975bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:23<00:00,  2.48it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():    \n",
    "    # State inputs\n",
    "    local_q = torch.tensor(sequence['local_q'], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    root_v = torch.tensor(sequence['root_v'], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    contact = torch.tensor(sequence['contact'], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    # Offset inputs\n",
    "    root_p_offset = torch.tensor(sequence['root_p_offset'], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    local_q_offset = torch.tensor(sequence['local_q_offset'], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    local_q_offset = local_q_offset.view(local_q_offset.size(0), -1)\n",
    "\n",
    "    # Target inputs\n",
    "    target = torch.tensor(sequence['target'], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    target = target.view(target.size(0), -1)\n",
    "\n",
    "    # Root position\n",
    "    root_p = torch.tensor(sequence['root_p'], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    # X\n",
    "    X = torch.tensor(sequence['X'], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    lstm.init_hidden(local_q.size(0))\n",
    "\n",
    "    root_pred = None\n",
    "    local_q_pred = None\n",
    "    contact_pred = None\n",
    "    root_v_pred = None\n",
    "    \n",
    "    for t in tqdm(range(sequence_length - 1)):\n",
    "        if t  == 0:\n",
    "            root_p_t = root_p[:,t]\n",
    "            local_q_t = local_q[:,t]\n",
    "            local_q_t = local_q_t.view(local_q_t.size(0), -1)\n",
    "            contact_t = contact[:,t]\n",
    "            root_v_t = root_v[:,t]\n",
    "        else:\n",
    "            root_p_t = root_pred[0]\n",
    "            local_q_t = local_q_pred[0]\n",
    "            contact_t = contact_pred[0]\n",
    "            root_v_t = root_v_pred[0]\n",
    "\n",
    "        state_input = torch.cat([local_q_t, root_v_t, contact_t], -1)\n",
    "\n",
    "        root_p_offset_t = root_p_offset - root_p_t\n",
    "        local_q_offset_t = local_q_offset - local_q_t\n",
    "        offset_input = torch.cat([root_p_offset_t, local_q_offset_t], -1)\n",
    "\n",
    "        target_input = target\n",
    "\n",
    "        h_state = state_encoder(state_input)\n",
    "        h_offset = offset_encoder(offset_input)\n",
    "        h_target = target_encoder(target_input)\n",
    "\n",
    "        tta = sequence_length - t - 2\n",
    "        \n",
    "        h_state += ztta[tta]\n",
    "        h_offset += ztta[tta]\n",
    "        h_target += ztta[tta]\n",
    "        \n",
    "#         if tta < 5:\n",
    "#             lambda_target = 0.0\n",
    "#         elif tta >= 5 and tta < 30:\n",
    "#             lambda_target = (tta - 5) / 25.0\n",
    "#         else:\n",
    "#             lambda_target = 1.0\n",
    "#         h_offset += 0.5 * lambda_target * torch.FloatTensor(h_offset.size()).normal_().to(device)\n",
    "#         h_target += 0.5 * lambda_target * torch.FloatTensor(h_target.size()).normal_().to(device)\n",
    "\n",
    "        h_in = torch.cat([h_state, h_offset, h_target], -1).unsqueeze(0)\n",
    "        h_out = lstm(h_in)\n",
    "\n",
    "        h_pred, contact_pred = decoder(h_out)\n",
    "        local_q_v_pred = h_pred[:, :, :88]\n",
    "        local_q_pred = local_q_v_pred + local_q_t\n",
    "\n",
    "        local_q_pred_ = local_q_pred.view(local_q_pred.size(0), local_q_pred.size(1), -1, 4)\n",
    "        local_q_pred_ = local_q_pred_ / torch.norm(local_q_pred_, dim = -1, keepdim = True)\n",
    "\n",
    "        root_v_pred = h_pred[:,:,88:]\n",
    "        root_pred = root_v_pred + root_p_t\n",
    "\n",
    "        pos_pred = skeleton.forward_kinematics(local_q_pred_, root_pred)\n",
    "\n",
    "        local_q_next = local_q[:,t+1]\n",
    "        local_q_next = local_q_next.view(local_q_next.size(0), -1)\n",
    "\n",
    "        # Saving images\n",
    "        plot_pose(np.concatenate([X[0, 0].view(22, 3).detach().cpu().numpy(),\\\n",
    "                                pos_pred[0, 0].view(22, 3).detach().cpu().numpy(),\\\n",
    "                                X[0, -1].view(22, 3).detach().cpu().numpy()], 0),\\\n",
    "                                t, './results/temp/pred')\n",
    "#         plot_pose(np.concatenate([X[0, 0].view(22, 3).detach().cpu().numpy(),\\\n",
    "#                                 X[0, t+1].view(22, 3).detach().cpu().numpy(),\\\n",
    "#                                 X[0, -1].view(22, 3).detach().cpu().numpy()], 0),\\\n",
    "#                                 t, './results/temp/gt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "513ac2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video ./results/sub1234_2sec_trial_4_epoch_200_01.mp4.\n",
      "Moviepy - Writing video ./results/sub1234_2sec_trial_4_epoch_200_01.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ./results/sub1234_2sec_trial_4_epoch_200_01.mp4\n"
     ]
    }
   ],
   "source": [
    "save_video(\"./results/temp/\", \"./results/sub1234_2sec_trial_4_epoch_200_01.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e9bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
