{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dac96f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from PIL import Image\n",
    "import moviepy.video.io.ImageSequenceClip\n",
    "from functions import gen_ztta\n",
    "from models import StateEncoder, OffsetEncoder, TargetEncoder, LSTM, Decoder\n",
    "from skeleton import Skeleton\n",
    "from LaFan import LaFan1\n",
    "from functions import gen_ztta\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e78c494",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2254fa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the data set... ['subject5']\n",
      "Processing file dance2_subject5.bvh\n",
      "Nb of sequences : 672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lafan = LaFan1(\"./lafan1/lafan1_small/flipped\", seq_len=data[\"seq_length\"], offset=10, train=False, debug=False)\n",
    "lafan_loader = DataLoader(lafan, batch_size=test[\"batch_size\"], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a424717d",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bedd261",
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton = Skeleton(offsets=data[\"offsets\"], parents=data[\"parents\"])\n",
    "skeleton.to(device)\n",
    "skeleton.remove_joints(data[\"joints_to_remove\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be12d5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_folder = \"./models/small_dataset_test_00/epoch_1/\"\n",
    "\n",
    "state_encoder = StateEncoder(in_dim=model[\"state_input_dim\"])\n",
    "state_encoder = state_encoder.to(device)\n",
    "state_encoder.load_state_dict(torch.load(f\"{models_folder}/state_encoder.pkl\", map_location=torch.device('cpu') ))\n",
    "\n",
    "offset_encoder = OffsetEncoder(in_dim=model[\"offset_input_dim\"])\n",
    "offset_encoder = offset_encoder.to(device)\n",
    "offset_encoder.load_state_dict(torch.load(f\"{models_folder}/offset_encoder.pkl\", map_location=torch.device('cpu') ))\n",
    "\n",
    "target_encoder = TargetEncoder(in_dim=model[\"target_input_dim\"])\n",
    "target_encoder = target_encoder.to(device)\n",
    "target_encoder.load_state_dict(torch.load(f\"{models_folder}/target_encoder.pkl\", map_location=torch.device('cpu') ))\n",
    "\n",
    "lstm = LSTM(in_dim=model[\"lstm_dim\"], hidden_dim=model[\"lstm_dim\"] * 2)\n",
    "lstm = lstm.to(device)\n",
    "lstm.load_state_dict(torch.load(f\"{models_folder}/lstm.pkl\", map_location=torch.device('cpu') ))\n",
    "\n",
    "decoder = Decoder(in_dim=model[\"lstm_dim\"]*2, out_dim=model[\"decoder_output_dim\"])\n",
    "decoder = decoder.to(device)\n",
    "decoder.load_state_dict(torch.load(f\"{models_folder}/decoder.pkl\", map_location=torch.device('cpu') ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44c63e9",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b066da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ztta = gen_ztta(length=data[\"seq_length\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fddfbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (fc0): Linear(in_features=1536, out_features=512, bias=True)\n",
       "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=91, bias=True)\n",
       "  (fc_conct): Linear(in_features=256, out_features=4, bias=True)\n",
       "  (ac_sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_encoder.eval()\n",
    "offset_encoder.eval()\n",
    "target_encoder.eval()\n",
    "lstm.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f4975bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_save = 20\n",
    "\n",
    "with torch.no_grad():\n",
    "    sampled_batch = iter(lafan_loader).next()\n",
    "    \n",
    "    # State inputs\n",
    "    local_q = sampled_batch['local_q'].to(device)\n",
    "    root_v = sampled_batch['root_v'].to(device)\n",
    "    contact = sampled_batch['contact'].to(device)\n",
    "\n",
    "    # Offset inputs\n",
    "    root_p_offset = sampled_batch['root_p_offset'].to(device)\n",
    "    local_q_offset = sampled_batch['local_q_offset'].to(device)\n",
    "    local_q_offset = local_q_offset.view(local_q_offset.size(0), -1)\n",
    "\n",
    "    # Target inputs\n",
    "    target = sampled_batch['target'].to(device)\n",
    "    target = target.view(target.size(0), -1)\n",
    "\n",
    "    # Root position\n",
    "    root_p = sampled_batch['root_p'].to(device)\n",
    "\n",
    "    # X\n",
    "    X = sampled_batch['X'].to(device)\n",
    "    \n",
    "    lstm.init_hidden(local_q.size(0))\n",
    "\n",
    "    root_pred = None\n",
    "    local_q_pred = None\n",
    "    contact_pred = None\n",
    "    root_v_pred = None\n",
    "    \n",
    "    for t in range(lafan.cur_seq_length - 1):\n",
    "        if t  == 0:\n",
    "            root_p_t = root_p[:,t]\n",
    "            local_q_t = local_q[:,t]\n",
    "            local_q_t = local_q_t.view(local_q_t.size(0), -1)\n",
    "            contact_t = contact[:,t]\n",
    "            root_v_t = root_v[:,t]\n",
    "        else:\n",
    "            root_p_t = root_pred[0]\n",
    "            local_q_t = local_q_pred[0]\n",
    "            contact_t = contact_pred[0]\n",
    "            root_v_t = root_v_pred[0]\n",
    "\n",
    "        state_input = torch.cat([local_q_t, root_v_t, contact_t], -1)\n",
    "\n",
    "        root_p_offset_t = root_p_offset - root_p_t\n",
    "        local_q_offset_t = local_q_offset - local_q_t\n",
    "        offset_input = torch.cat([root_p_offset_t, local_q_offset_t], -1)\n",
    "\n",
    "        target_input = target\n",
    "\n",
    "        h_state = state_encoder(state_input)\n",
    "        h_offset = offset_encoder(offset_input)\n",
    "        h_target = target_encoder(target_input)\n",
    "\n",
    "        h_state += ztta[:, t]\n",
    "        h_offset += ztta[:, t]\n",
    "        h_target += ztta[:, t]\n",
    "\n",
    "        h_in = torch.cat([h_state, h_offset, h_target], -1).unsqueeze(0)\n",
    "        h_out = lstm(h_in)\n",
    "\n",
    "        h_pred, contact_pred = decoder(h_out)\n",
    "        local_q_v_pred = h_pred[:, :, :88]\n",
    "        local_q_pred = local_q_v_pred + local_q_t\n",
    "\n",
    "        local_q_pred_ = local_q_pred.view(local_q_pred.size(0), local_q_pred.size(1), -1, 4)\n",
    "        local_q_pred_ = local_q_pred_ / torch.norm(local_q_pred_, dim = -1, keepdim = True)\n",
    "\n",
    "        root_v_pred = h_pred[:,:,88:]\n",
    "        root_pred = root_v_pred + root_p_t\n",
    "\n",
    "        pos_pred = skeleton.forward_kinematics(local_q_pred_, root_pred)\n",
    "\n",
    "        local_q_next = local_q[:,t+1]\n",
    "        local_q_next = local_q_next.view(local_q_next.size(0), -1)\n",
    "\n",
    "        # Saving images\n",
    "        plot_pose(np.concatenate([X[sample_to_save,0].view(22, 3).detach().cpu().numpy(),\\\n",
    "                                pos_pred[0, sample_to_save].view(22, 3).detach().cpu().numpy(),\\\n",
    "                                X[sample_to_save,-1].view(22, 3).detach().cpu().numpy()], 0),\\\n",
    "                                t, './results/temp/pred')\n",
    "        plot_pose(np.concatenate([X[sample_to_save,0].view(22, 3).detach().cpu().numpy(),\\\n",
    "                                X[sample_to_save,t+1].view(22, 3).detach().cpu().numpy(),\\\n",
    "                                X[sample_to_save,-1].view(22, 3).detach().cpu().numpy()], 0),\\\n",
    "                                t, './results/temp/gt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "513ac2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video ./results/small_dataset_test_00.mp4.\n",
      "Moviepy - Writing video ./results/small_dataset_test_00.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ./results/small_dataset_test_00.mp4\n"
     ]
    }
   ],
   "source": [
    "save_video(\"./results/temp/\", \"./results/small_dataset_test_00.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d662408",
   "metadata": {},
   "source": [
    "# Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f07e1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pose(pose, cur_frame, prefix):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    parents = [-1, 0, 1, 2, 3, 0, 5, 6, 7, 0, 9, 10, 11, 12, 11, 14, 15, 16, 11, 18, 19, 20]\n",
    "    ax.cla()\n",
    "    num_joint = pose.shape[0] // 3\n",
    "    for i, p in enumerate(parents):\n",
    "        if i > 0:\n",
    "            ax.plot([pose[i, 0], pose[p, 0]],\\\n",
    "                    [pose[i, 2], pose[p, 2]],\\\n",
    "                    [pose[i, 1], pose[p, 1]], c='r')\n",
    "            ax.plot([pose[i+num_joint, 0], pose[p+num_joint, 0]],\\\n",
    "                    [pose[i+num_joint, 2], pose[p+num_joint, 2]],\\\n",
    "                    [pose[i+num_joint, 1], pose[p+num_joint, 1]], c='b')\n",
    "            ax.plot([pose[i+num_joint*2, 0], pose[p+num_joint*2, 0]],\\\n",
    "                    [pose[i+num_joint*2, 2], pose[p+num_joint*2, 2]],\\\n",
    "                    [pose[i+num_joint*2, 1], pose[p+num_joint*2, 1]], c='g')\n",
    "    # ax.scatter(pose[:num_joint, 0], pose[:num_joint, 2], pose[:num_joint, 1],c='b')\n",
    "    # ax.scatter(pose[num_joint:num_joint*2, 0], pose[num_joint:num_joint*2, 2], pose[num_joint:num_joint*2, 1],c='b')\n",
    "    # ax.scatter(pose[num_joint*2:num_joint*3, 0], pose[num_joint*2:num_joint*3, 2], pose[num_joint*2:num_joint*3, 1],c='g')\n",
    "    xmin = np.min(pose[:, 0])\n",
    "    ymin = np.min(pose[:, 2])\n",
    "    zmin = np.min(pose[:, 1])\n",
    "    xmax = np.max(pose[:, 0])\n",
    "    ymax = np.max(pose[:, 2])\n",
    "    zmax = np.max(pose[:, 1])\n",
    "    scale = np.max([xmax - xmin, ymax - ymin, zmax - zmin])\n",
    "    xmid = (xmax + xmin) // 2\n",
    "    ymid = (ymax + ymin) // 2\n",
    "    zmid = (zmax + zmin) // 2\n",
    "    ax.set_xlim(xmid - scale // 2, xmid + scale // 2)\n",
    "    ax.set_ylim(ymid - scale // 2, ymid + scale // 2)\n",
    "    ax.set_zlim(zmid - scale // 2, zmid + scale // 2)\n",
    "\n",
    "    plt.draw()\n",
    "    plt.savefig(f\"{prefix}_{cur_frame:02}.png\", dpi=200, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9312bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video(frames_loc, filepath):\n",
    "    fps = 30\n",
    "    \n",
    "    frames = [os.path.join(frames_loc, img) for img in os.listdir(frames_loc) if img.endswith(\".png\") and img.startswith(\"pred\")]\n",
    "    clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(frames, fps=fps)\n",
    "    clip.write_videofile(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e854058e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
